#version 450
#extension GL_EXT_mesh_shader : require
#extension GL_EXT_shader_explicit_arithmetic_types_int8 : require
#extension GL_EXT_samplerless_texture_functions : require

#if defined(VARIANT_BIT_2) && VARIANT_BIT_2
#define MESHLET_PRIMITIVE_CULL_WAVE32 1
#else
#define MESHLET_PRIMITIVE_CULL_WAVE32 0
#endif

#if defined(VARIANT_BIT_0) && VARIANT_BIT_0
#define MESHLET_RENDER_TASK_HIERARCHICAL 1
#define WG_Y 4
#else
#define MESHLET_RENDER_TASK_HIERARCHICAL 0
#define WG_Y 1
#endif

#if defined(VARIANT_BIT_3) && VARIANT_BIT_3
#define MESHLET_RENDER_PHASE 1
#elif defined(VARIANT_BIT_4) && VARIANT_BIT_4
#define MESHLET_RENDER_PHASE 2
#else
#define MESHLET_RENDER_PHASE 0
#endif

layout(local_size_x = 32, local_size_y = WG_Y) in;

#if defined(VARIANT_BIT_5) && VARIANT_BIT_5
#define MESHLET_RENDER_FORCE_VISIBLE
#endif

#if MESHLET_PRIMITIVE_CULL_WAVE32
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_shuffle : require
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_arithmetic : require
#endif

#include "inc/meshlet_render.h"
#include "inc/render_parameters.h"

layout(constant_id = 1) const bool SKINNED = false;

layout(push_constant, std430) uniform Registers
{
    uint offset;
    uint count;
} registers;

taskPayloadSharedEXT CompactedDrawInfoPayload mesh_payload;

#if MESHLET_RENDER_TASK_HIERARCHICAL
shared uint payload_offset;
shared uint precull_offset;
shared uint preculled_offsets[128];
#else
uint payload_offset = 0;
#endif

shared uint ballot_value[4];

uvec4 ballot(bool v)
{
#if MESHLET_PRIMITIVE_CULL_WAVE32
    if (gl_SubgroupSize == 32)
        return subgroupBallot(v);
#endif

    barrier();
    if (gl_LocalInvocationID.x == 0)
        ballot_value[gl_LocalInvocationID.y] = 0;
    barrier();
    if (v)
        atomicOr(ballot_value[gl_LocalInvocationID.y], 1u << gl_LocalInvocationID.x);
    barrier();
    return uvec4(ballot_value[gl_LocalInvocationID.y], 0, 0, 0);
}

uint ballotBitCount(uvec4 v)
{
#if MESHLET_PRIMITIVE_CULL_WAVE32
    if (gl_SubgroupSize == 32)
        return subgroupBallotBitCount(v);
#endif

    return bitCount(v.x);
}

uint ballotExclusiveBitCount(uvec4 v)
{
#if MESHLET_PRIMITIVE_CULL_WAVE32
    if (gl_SubgroupSize == 32)
        return subgroupBallotExclusiveBitCount(v);
#endif

    return bitCount(bitfieldExtract(v.x, 0, int(gl_LocalInvocationID.x)));
}

uint get_local_index()
{
    uint local_index = gl_LocalInvocationID.x;
#if MESHLET_PRIMITIVE_CULL_WAVE32
    if (gl_SubgroupSize == 32)
        local_index = gl_SubgroupInvocationID;
#endif

    return local_index;
}

uint get_subgroup_index()
{
    uint local_index = gl_LocalInvocationID.y;
#if MESHLET_PRIMITIVE_CULL_WAVE32
    if (gl_SubgroupSize == 32)
        local_index = gl_SubgroupID;
#endif

    return local_index;
}

void process_task(uint task_index)
{
    MeshAssetDrawTaskInfo task = task_info.data[task_index];

    uint node_instance = task.node_instance;
    uint material_flags = task.material_flags;
    uint mesh_index_count = task.mesh_index_count;

    uint offset = mesh_index_count & ~31u;
    uint count = bitfieldExtract(mesh_index_count, 0, 5) + 1;
    uint local_index = get_local_index();
    uint meshlet_index = offset + local_index;

#if MESHLET_RENDER_PHASE >= 1
    uint visibility_state = occluders.data[task.occluder_state_offset];
#endif

    bool alloc_draw = false;
    if (local_index < count)
    {
        mat_affine M = transforms.data[node_instance];
        Bound b = bounds.data[meshlet_index];

#if MESHLET_RENDER_PHASE == 1
        alloc_draw = bitfieldExtract(visibility_state, int(local_index), 1) != 0;
#else
        alloc_draw = true;
#endif

        if (alloc_draw)
        {
            if (SKINNED)
            {
#if !MESHLET_RENDER_TASK_HIERARCHICAL
                // We already did the frustum cull at hierarchical level.
                // It's not very efficient to do it like this on non-AMD.
                // TODO: We'd need some way to make skinning + cluster bounds work.
                AABB aabb = aabb.data[task.aabb_instance];
                alloc_draw = frustum_cull(aabb.lo, aabb.hi);
#endif
            }
            else
            {
                alloc_draw = cluster_cull(M, b, global.camera_position);
            }
        }
    }

    uvec4 draw_ballot = ballot(alloc_draw);

#if MESHLET_RENDER_PHASE == 2
    // Record all clusters that are considered visible in this frame.
    if (local_index == 0)
        occluders.data[task.occluder_state_offset] = draw_ballot.x;

#ifndef MESHLET_RENDER_FORCE_VISIBLE
    // If we already rendered in phase 1, skip the draw here.
    // If we only did a Z-prepass, we may still need to render to fill in the attachments properly.
    // Doing Z-prepass helps mask bad task shader behavior on at least AMD.
    if (bitfieldExtract(visibility_state, int(local_index), 1) != 0)
        alloc_draw = false;
#endif
#endif

#if MESHLET_RENDER_TASK_HIERARCHICAL
    if (alloc_draw)
        mesh_payload.task_offset_mesh_offsets[atomicAdd(payload_offset, 1u)] = (task_index << 5) | local_index;
#else
#ifndef MESHLET_RENDER_FORCE_VISIBLE
    draw_ballot = ballot(alloc_draw);
#endif
    uint draw_count = ballotBitCount(draw_ballot);
    uint local_offset = ballotExclusiveBitCount(draw_ballot);
    if (local_index == 0)
        mesh_payload.info = CompactedDrawInfo(offset, node_instance, material_flags);
    if (alloc_draw)
        mesh_payload.offsets[local_offset] = uint8_t(local_index);
    payload_offset += draw_count;
#endif
}

void main()
{
#if MESHLET_RENDER_TASK_HIERARCHICAL
    if (gl_LocalInvocationIndex == 0)
    {
        payload_offset = 0;
        precull_offset = 0;
    }

    uint global_flat_index = gl_WorkGroupID.x * gl_WorkGroupSize.x * gl_WorkGroupSize.y + gl_LocalInvocationIndex;
    uint task_index = registers.offset + global_flat_index;
    bool task_needs_work = false;

    if (global_flat_index < registers.count)
    {
        MeshAssetDrawTaskInfo task = task_info.data[task_index];

        // Cull the group.
        AABB aabb = aabb.data[task.aabb_instance];
        bool visible = frustum_cull(aabb.lo, aabb.hi);

#if MESHLET_RENDER_PHASE == 1
        task_needs_work = visible && occluders.data[task.occluder_state_offset] != 0;
#else
        task_needs_work = visible;
#if MESHLET_RENDER_PHASE == 2
        // Nothing is visible, clear out occluder state.
        if (!task_needs_work)
            occluders.data[task.occluder_state_offset] = 0;
#endif
#endif
    }

    barrier();

    if (task_needs_work)
        preculled_offsets[atomicAdd(precull_offset, 1u)] = task_index;

    barrier();

    uint count = precull_offset;
    for (uint index = get_subgroup_index(); index < count; index += gl_WorkGroupSize.y)
        process_task(preculled_offsets[index]);
#else
    uint task_index = gl_WorkGroupID.x + registers.offset;
    process_task(task_index);
#endif

#if MESHLET_RENDER_TASK_HIERARCHICAL
    barrier();
#endif
    EmitMeshTasksEXT(8, payload_offset, 1);
}
